---
title: 09. 웹 크롤러 설계
sidebar_position: 9
tag: [book, system design interview]
---
## 웹 크롤러란?
- 웹에 올라오거나 갱신된 콘텐츠를 찾아내는 목적을 가진 프로그램
- 여기서 콘텐츠는 다양한 형식일 수 있음(web page, pdf, image ...)
- 사용 예시: 검색 엔진 인덱싱, 웹 아카이빙, 웹 마이닝, 웹 모니터링

## 설계 시 고민할 것들
- 웹 크롤러의 목적
- 수집이 필요한 웹 페이지의 규모
- 웹 페이지 형식 변화에 대한 대응 여부
- 웹 페이지 저장 여부 및 보관 기간
- 중복된 컨텐츠 처리 여부 등...
- 규모 확장성, 안정성, 수집 대상 웹 페이지 요청 수, 수집 콘텐츠의 확장성 등등...

## 웹 크롤러 개략적인 설계도
![web_crawler.jpg](img/web_crawler.jpg)
#### 1. 시작 URL 집합
- 웹 크롤러가 크롤링을 시작하는 출발점들의 집합
- 대개 수집할 전체 URL 공간을 작은 부분집합으로 나누는 전략을 사용한다. (지역, 주제 등등..)

#### 2. 미수집 URL 저장소
- 크롤링 상태를 관리하기 위해 URL을 다운로드 할 URL / 다운로드된 URL로 나눈다.
- 이 중에서 다운로드 할 URL을 저장 관리하는 컴포넌트가 미수집 URL 저장소이다.

#### 3. HTML 다운로더
- 말 그대로 URL을 인자로 받아 콘텐츠를 다운로드하는 컴포넌트이다.

#### 4. 도메인 이름 변환기
- URL을 대응되는 IP로 변환하는 컴포넌트이다.


#### 5. 콘텐츠 파서
- 다운받은 콘텐츠를 parsing하고 validation하는 역할을 담당한다.
- 원하는 형식의 데이터를 걸러내는 작업을 진행한다.
- 크롤링 서버 내에 콘텐츠 파서를 구현하면 크롤링 과정이 느려짐으로 별도의 컴포넌트로 두는 것도 고려할 수 있다.

#### 6. 중복 콘텐츠 여부
- 책에 의하면 29%가량의 웹 콘텐츠는 중복이라고 한다.
- 때문에 적합한 자료 구조를 도입하여 중복과 데이터 처리 소요 시간을 줄이는 컴포넌트이다.
- HTML 비교에 가장 직관적인 방법은 문자열 비교이겠지만, 큰 규모에서 성능을 고려한다면 책에서는 Hash값 비교를 추천한다.

#### 7. 콘텐츠 저장소
- 수집한 콘텐츠를 저장하는 시스템
- 콘텐츠 유형, 크기, 접근 빈도, 보관 기간 등을 고려하여 선택할 수 있다.
- 책에서는 디스크와 메모리를 동시에 사용하는 저장소를 택했다.

#### 8. URL 추출기
- HTML 페이지를 파싱하여 링크들을 골라내는 역할을 한다.
- 예를 들면 상대 경로에 도메인을 추가하여 절대 경로로 변환하는 역할이다.

#### 9. URL 필터
- 특정한 콘텐츠 타입나 파일 확장자, 오류가 발생하는 URL, 접근 제외 목록에 포함된 URL을 크롤링 대상에서 제외하는 역할을 담당한다.

#### 10. 이미 방문한 URL 여부
- 다운로드 할 URL / 다운로드된 URL을 바탕으로 이미 방문한 URL을 거르는 컴포넌트이다.
- 책에서는 자료 구조를 통해 다운로드 할 URL / 다운로드된 URL를 관리한다.
- 해당 컴포넌트를 통해 서버 주하를 줄이고 무한 루프를 방지할 수 있다.
- 자료 구조로는 블룸 필터 혹은 해시 테이블이 널리 쓰인다.

#### 11. URL 저장소
- 이미 방문한 URL을 보관하는 저장소이다.